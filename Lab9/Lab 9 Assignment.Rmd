---
title: "R Notebook"
output: html_notebook
---

```{r, warning=F, message=F}

rm(list=ls())

require(sf)
require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(PresenceAbsence)
require(ecospat)
```

# This first code chunk just recreates the maps we built in the lab.

```{r}

# Model building data
vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))



# Validation data
vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))



# Bringing in the covariates
elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')


# Resampling to make the covariate rasters match
mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

# Mesic forest within 1 km
probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')


# Building the raster stack
layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')


#Creating background points
set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Extracting covariates for our different points
presCovs = extract(layers, vathPresXy)
absCovs = extract(layers, vathAbsXy)
backCovs = extract(layers, backXy)
valCovs = extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
absCovs = data.frame(vathAbsXy, absCovs, pres=0)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

presCovs = presCovs[complete.cases(presCovs),]
absCovs = absCovs[complete.cases(absCovs),]
backCovs = backCovs[complete.cases(backCovs),]

# Combining presence and background data into one dataframe

backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')
colnames(absCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)
presAbsCovs = rbind(presCovs, absCovs)

valCovs = valCovs %>% mutate(VATH = vathVal$VATH)
valCovs = valCovs[complete.cases(valCovs),]


# Fitting bioclim envelope model
tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)



# Fitting GLM
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')


# Fitting GAM
gamModel = gam(pres ~ s(canopy, k=6) + s(elev, k=6) + s(mesic1km, k=6) + s(precip, k=6), family='binomial', data=presBackCovs, method='ML')

gamMap = predict(layers, gamModel, type='response')


# Fitting boosted regression tree model

boostModel = gbm(pres ~ elev + canopy + mesic1km + precip, distribution='bernoulli', n.trees=100, interaction.depth=2, shrinkage=0.1, bag.fraction=0.5, data=presBackCovs)

boostMap = predict(layers, boostModel, type='response')
boostMap = mask(boostMap, layers$canopy)


# Fitting random forest model

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)


#Fitting maxent model

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')


maxentMap = predictMaxNet(maxentModel, layers, type='logistic')
```



# Challenge 1 (4 points)

In the lab, we fit 6 SDMs. We then calculated discrimination statistics for all 6 and a calibration plot for 1 of them. Create calibration plots for the remaining 5 models, and then make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best." Defend your answer.

```{r}

# Replicating the discrimination stats from the lab

# Temp data

tmp = valCovs 

# New data.frame

valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1])

# the actual stats

summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData[,2], valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData[,i+2]*valData[,2] + (1-valData[,i+2]) * (1-valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData[,i+2] + 0.01)*valData[,2] + log((1-valData[,i+2]))*(1-valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData)[3:8])

summaryEval

# calibration plots

par(mar = c(4, 4, 0.5, 0.1), mfrow = c(2, 3))

calibration.plot(valData, which.model=1, N.bins=20, xlab='bio', ylab='Observed', main='')

calibration.plot(valData, which.model=2, N.bins=20, xlab='glm', ylab='Observed', main='')

calibration.plot(valData, which.model=3, N.bins=20, xlab='gam', ylab='Observed', main='')

calibration.plot(valData, which.model=4, N.bins=20, xlab='boost', ylab='Observed', main='')

calibration.plot(valData, which.model=5, N.bins=20, xlab='rf', ylab='Observed', main='')

calibration.plot(valData, which.model=6, N.bins=20, xlab='maxent', ylab='Observed', main='')


```
Answer the question here.


Make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best."

Below are the top three performing models (upper 50th percentile) for each statistic without considering the gap between each:

AUC - GLM, MaxEnt, GAM
Corr - GLM, MaxEnt, GAM
Likelihood - GLM, Boost, GAM
Sensitivity - Boost, GAM, Bio
Specificity - MaxEnt, GLM, RF
TSS - GLM, MaxEnt, GAM
Kappa - MaxEnt, GLM, GAM
Calibration plot (visually)- GAM, GLM, Boost

I chose to display the top three performing models for each stat like this because I think that it really highlights the relative strengths and weaknesses of each model. I think the most interesting trend is with sensitivity and specificity. The top three performing models for one was the bottom three for the other. While they were not necessarily in order (i.e., the best performing model at one was not the worst performing model at the other), I think that it shows the trade-off between accurately predicting where a species should be versus accurately predicting where it should not be (and I know there are a lot of complications with assumptions here, like the species being everywhere that is suitable). The other statistics kind of reflect this same trade-off. Using the three primary discrimination statistics (AUC, Kappa, and TSS), it is pretty clear that the GLM and MaxEnt models have performed the best (with the GAM model being a relatively close 3rd). Using our two calibration methods (likelihood and calibration plots), the GLM model is clearly the best performing model (with GAM and Boost pretty distant from GLM but close to each other, like a 2a and 2b). Therefore, I would have to conclude that the GLM model is the "best" SDM model out of this model suite, using our data.

# Challenge 2 (4 points)

Each SDM we created uses a different algorithm with different assumptions. Because of this, ecologists frequently use "ensemble" approaches that aggregate predictions from multiple models in some way. Here we are going to create an ensemble model by calculating a weighted average of the predicted occupancy values at each pixel. We will calculate weights based on model AUC values to ensure that the models with the best AUC values have the most influence on the predicted values in the ensemble model.

Create a raster stack that combines the glmMap, gamMap, boostMap, and rfMap (hint use c()).

Next, create a vector of the AUC values for each model.

Lastly, use the weighted.mean() function in the terra package to create the new raster as a weighted average of the previous 4 rasters.

Plot the result, and explain why we left out the bioclim and Maxent models for this ensemble model.

```{r}
# raster stack

ensemble <- stack(c(glmMap, gamMap, boostMap, rfMap))

plot(ensemble)

# AUC values for each model (I think this is what you are asking for, so that we can weight by AUC??)

weights <- summaryEval$auc[2:5]

# Weighted mean

wm <- terra::weighted.mean(x = ensemble, w = weights)

plot(wm)

```

Answer the question here.

Explain why we left out the bioclim and Maxent models for this ensemble model.

To the best of my knowledge, the bioclim and MaxEnt models should be left out because they are specifically designed to use presence-only data. This is why I think that they performed so poorly in the sensitivity, log-likelihood and calibration plots. Its hard to know where a species should not be, if the model is not designed to clarify where the species has not been found. For example, I know that MaxEnt does not assume that the background points are true absences. Furthermore, the bioclim model only uses presence. Again, that is why I think that both of these models tend to overestimate presence (as seen in calibration plots, sensitivity, and likelihood).

# Challenge 3 (4 points)

Is this ensemble model an improvement over one of the models you built previously? Provide evidence and explain the criteria you used to come to your conclusion.

```{r}

# extract from ensemble and make df

tmp = valCovs

extrCoor = data.frame("EASTING" = tmp[,1], "NORTHING" = tmp[,2])

wmVal = extract(wm, extrCoor)

# lets put everything together and look at the stats so we don't have to bounce around (I thought about making the if-loop a function, but I do not know how well those cross over between versions or computers, so I apologize for this code being longer than it probably should be)

valData2 = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1],
         ensembleVal = wmVal)

summaryEval2 = data.frame(matrix(nrow=0, ncol=10))

nModels = ncol(valData2)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData2, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData2, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData2, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData2, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData2, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData2[,2], valData2[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData2[,i+2]*valData2[,2] + (1-valData2[,i+2]) * (1-valData2[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData2[,i+2] + 0.01)*valData2[,2] + log((1-valData2[,i+2]))*(1-valData2[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval2 = rbind(summaryEval2, summaryI)
}

summaryEval2 = summaryEval2 %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData2)[3:9])

summaryEval2

# calibration plots (man, these are hard to work with #ggplot4life)

par(oma = c(0, 0, 0, 0), omi = c(0, 0, 0, 0), mar = c(2.5, 4, 0, 0.1), mfrow = c(2, 4))

calibration.plot(valData2, which.model=1, N.bins=20, xlab='bio', ylab='Observed', model.names = NULL, main = "")

calibration.plot(valData2, which.model=2, N.bins=20, xlab='glm', ylab='', model.names = NULL, main = "")

calibration.plot(valData2, which.model=3, N.bins=20, xlab='gam', ylab='', model.names = NULL, main = "")

calibration.plot(valData2, which.model=4, N.bins=20, xlab='boost', ylab='', model.names = NULL, main = "")

calibration.plot(valData2, which.model=5, N.bins=20, xlab='rf', ylab='Observed', model.names = NULL, main = "")

calibration.plot(valData2, which.model=6, N.bins=20, xlab='maxent', ylab='', model.names = NULL, main = "")

calibration.plot(valData2, which.model=7, N.bins=20, xlab='ensemble', ylab='', model.names = NULL, main = "")


```

Is this ensemble model an improvement over one of the models you built previously?

Below are where the ensemble model ranked among the model suite with various statistics (and visually from the calibration plots):

AUC - very close 2nd
Corr - 2nd 
Likelihood - 1st
Sensitivity - 4th
Specificity - 3rd
TSS - 3rd (2nd and 3rd very close)
Kappa - 3rd
Calibration plot- ensemble better than bio, boost, rf, and maxent. Worse than glm and gam.


With this information, I think that it is safe to conclude that the ensemble model did not improve our overall efforts as it did not consistently out-perform all other models. However, the ensemble model did consistently outperform several of the models, including the GAM, boosted, RF, and bio models. Other original models are obviously the same as before (the test data did not change). When adding the ensemble model into the suite, it does perform relatively well compared to the entire suite of models, but it is not the best model, so I do not think that it adds value over the other best-performing models.

# Challenge 4 (4 points)

In the lab we built models using presence-background data then validated those models with presence-absence data. For this challenge, you're going to compare the predictive ability of a model built using presence-background data with one built using presence-absence data.

Fit a GLM using the presence-background data as we did in the lab (i.e., use the presBackCovs dataframe). Fit a second GLM using the presence-absence data (i.e., use the presAbsCovs dataframe). Validate both of these models on the novel presence-absence data (valCovs dataset). Specifically, calculate and compare AUC, Kappa, and TSS for these two models. Which model does a better job of prediction for the validation data and why do you think that is? 

```{r}

# This is the presence-background (original) GLM, I have re-coded it here so that everything is together

glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')

# Presence-absence GLM

abGLM = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presAbsCovs)

abglmMap = predict(layers, abGLM, type='response')

# Validation data

tmp = valCovs

valData3 = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         backVal = predict(glmModel, tmp %>% select(canopy:precip), type = 'response'),
         abVal = predict(abGLM, tmp %>% select(canopy:precip), type='response'))

# Evaluate

summaryEval3 = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData3)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData3, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData3, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData3, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData3, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData3, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData3[,2], valData3[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData3[,i+2]*valData3[,2] + (1-valData3[,i+2]) * (1-valData3[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData3[,i+2] + 0.01)*valData3[,2] + log((1-valData3[,i+2]))*(1-valData3[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval3 = rbind(summaryEval3, summaryI)
}

summaryEval3 = summaryEval3 %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData3)[3:4])

summaryEval3

# Calibration plots

par(oma = c(0, 0, 0, 0), omi = c(0, 0, 0, 0), mar = c(2.5, 4, 0, 0.1), mfrow = c(1, 2))

calibration.plot(valData3, which.model=1, N.bins=20, xlab='Background', ylab='Observed', model.names = NULL, main = "")

calibration.plot(valData3, which.model=2, N.bins=20, xlab='Absence', ylab='', model.names = NULL, main = "")

```

Which model does a better job of prediction for the validation data and why do you think that is?

Below are which model performed better according to each statistic (and visually from the calibration plots):

AUC - presence-absence
Corr - presence-absence
Likelihood - presence-background
Sensitivity - presence-absence
Specificity - presence-background
TSS - presence-absence
Kappa - presence-background
Calibration plot (visually)- presence-background

Again, I think that this comes down to the differences between sensitivity and specificity. The presence-absence model over-predicted suitability, so it performed well based on some statistics and poorly based on others. Based on the three statistics that you said to compare specifically (i.e., AUC, Kappa, and TSS), I would conclude that the presence-absence model is best as it performed better based on two of those three stats. When everything is taken into consideration, however, it becomes murky and the presence-background model is probably better supported.


# Challenge 5 (4 points)

Now calculate the same statistics (AUC, Kappa, and TSS) for each model you developed in Challenge 4 using K-fold validation with 5 groups. Do these models perform better or worse based on K-fold validation (as compared to validation based on novel data)? Why might that occur?

```{r}
# Break into groups

set.seed(23)

nFolds = 5
kfoldPres = kfold(presCovs, k=nFolds)
kfoldBack = kfold(backCovs, k=nFolds)
kfoldAbs = kfold(absCovs, k=nFolds)

# K-folds for presence-background

boyceVals = rep(NA, nFolds)

for(i in 1:nFolds){
  valPres = presCovs[kfoldPres==i,]
  
  trainPres = presCovs[kfoldPres!=i,]
  trainBack = backCovs[kfoldBack!=i,]
  trainBoth = rbind(trainPres, trainBack)
  
  glmModel2 = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth)

  valData3 = data.frame('ID' = 1:nrow(valPres)) %>% 
  mutate(obs = valPres$pres,
         backVal = predict(glmModel2, valPres %>% select(canopy:precip), type = 'response'))
  
boyceVals[i] = ecospat.boyce(fit = glmMap, obs=valData3[,3], res=100, PEplot=F)$cor

}

mean(boyceVals)

# K-folds for presence-absence

boyceVals = rep(NA, nFolds)

for(i in 1:nFolds){
  valPres = presCovs[kfoldPres==i,]
  
  trainPres = presCovs[kfoldPres!=i,]
  trainAbs = absCovs[kfoldAbs!=i,]
  trainBoth = rbind(trainPres, trainAbs)
  
  abGLM2 = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth)

  valData3 = data.frame('ID' = 1:nrow(valPres)) %>% 
  mutate(obs = valPres$pres,
         abVal = predict(abGLM2, valPres %>% select(canopy:precip), type='response'))
  
boyceVals[i] = ecospat.boyce(fit = abglmMap, obs=valData3[,3], res=100, PEplot=F)$cor

}

mean(boyceVals)

```

Do these models perform better or worse based on K-fold validation (as compared to validation based on novel data)? Why might that occur?

Well, I do not really know if the models performed better based on K-fold when compared to other validation because I do not know how those stats typically translate to each other. However, based on the fact that they range from -1 to 1, I would have to assume that they did perform better via the K-fold method as compared to Kappa and TSS. I would imagine that this is the case because of some slight differences between suitability between the two years of data collection (training versus validation data). Theoretically, rather small changes in suitability across all sites could result in pretty stark differences in model fit, given our large sample size. Overall, the presence-background model performed better than the presence-absence model. Given what we saw before (i.e., the presence-absence model tended to over-predict suitability), I would have actually predicted the opposite, since we were using presence only to validate the models in this challenge. Perhaps, since the presence-absence model was over-predicting suitability, it was better able to handle the differences in relative suitability between the two sampling years, but when training based on the same year, this trend was actually detrimental to the performance of the model (based on K-fold).
