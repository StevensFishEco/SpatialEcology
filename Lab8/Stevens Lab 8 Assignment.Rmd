---
title: "R Notebook"
output: html_notebook
---

```{r}
# Packages from the lab

require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(landscapemetrics)

```



# Challenge 1 (4 points)

In the lab, we created 6 species distribution models (SDMs) for the same species using 6 different techniques. Plot the maps generated from (1) the bioclim envelope function, (2) the GLM model, and (3) the random forest model next to one another. What similarities and differences do you notice among these maps? What might explain some of these differences?

```{r}
# Lets get the occurrence data

vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))

# Validation data

vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))

# Environmental covariates 

elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')

crs(elev) = crs(mesic)
crs(canopy) = crs(mesic)

mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')

layers = c(canopy, elev, mesic, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic', 'mesic1km', 'precip')

layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')

# Background points 

set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Covariates by sample

presCovs = extract(layers, vathPresXy)
backCovs = extract(layers, backXy)
valCovs = extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

# Combine present and background

presCovs = presCovs[complete.cases(presCovs),]
backCovs = backCovs[complete.cases(backCovs),]
valCovs = valCovs[complete.cases(valCovs),]


backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)

# Bioclim enevlope function

tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)
plot(bioclimMap)

# GLM model

glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

summary(glmModel)

glmMap = predict(layers, glmModel, type='response')
plot(glmMap)

# Random forest model

tuneRF(y = as.factor(presBackCovs$pres), x=presBackCovs[,3:6], stepFactor = 2, ntreeTry = 500)

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)
plot(rfMap)



```

Q. What similarities and differences do you notice among these maps? What might explain some of these differences? 

So, in general, the same areas are highlighted as suitable habitat in all three methods. The large block in the center and the northwest corner are moderate to highly suitable habitat in all methods. Additionally, the eastern third is primarily low quality habitat in all three methods. Within those generalities, there are some differences in the abundance nad distribution of predicted quality habitat. For example, the GLM model results in a much larger amount of highly suitable habitat in the center block than the other methods. Interestingly, however, the GLM model has a much lower scale than the other two methods (i.e., up to ~0.4 versus > 0.8), so I do not actually know how that difference in quality as perceived from the maps compares between the methods. As to why they're different, I imagine its because they're calculating suitability with entirely different methods. It makes sense that the GLM model has the lowest predicted values because it should have the lowest predictive abilities. I think the power of the bioclim and random forest models is that they can kind of "ignore" some of the noise associated with the occurrence data. The bioclim does this very obviously by the cutoff of the box boundaries. The random forest is a little less obvious but by using multiple trees, it can tease out of the occurrences in areas with covariate values that may not agree with the overall average. Essentially, since the bioclim and random forest models creates the quasi-boundaries for each covariate, the models actually have thresholds for each covariate and as long as a site meets those thresholds, suitability should be high. GLM on the other hand has much less ability to handle occurrences in "odd" areas. Additionally, I suspect that the random forest model handles the background points much better than the GLM (bioclim doesnlt have background at all). So that may also explain the difference in predictive abilities (for the same reasons as above).

# Challenge 2 (4 points)

When we fit our GLM in lab, we used background points, rather than true absence points, to represent pseudo-absences. Fit the exact same GLM model, only this time use presence and true absence data. That is, replace the background rows in the dataframe with rows that represent actual sites where surveys were completed but Varied Thrush were not detected. Once you've fit the GLM, build a new SDM from this fitted model and visually compare the prediction surface to that built based on the presence-background model. What discrepancies do you notice, and what is your intuition regarding which of these two models is more reliable?

```{r}
# Lets get the data frame how we want it

absCovs = extract(layers, vathAbsXy)

absCovs = data.frame(vathAbsXy, absCovs, pres=0)

absCovs = absCovs[complete.cases(absCovs),]

colnames(absCovs)[1:2] = c('x', 'y')

paCovs = rbind(presCovs, absCovs)

# GLM model

paglmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=paCovs)

summary(paglmModel)

paglmMap = predict(layers, paglmModel, type='response')
plot(paglmMap)

plot(glmMap)
```

Q. What discrepancies do you notice, and what is your intuition regarding which of these two models is more reliable?

Again, the scale is different so these are relative discrepancies (the actual values are difficult to determine between the two maps). Anyway, the biggest difference is the amount of highly suitable habitat. The large block in the center and the northwest corner has become much more suitable in the new GLM. Additionally, the eastern half resembles the output of the other two methods much closer with the new GLM. Additionally, the southwestern corner now resembles the other two methods as well. The new GLM resembling the other two methods more closly (except from the overall amount of suitable habitat, which is much higher with the GLM model than bioclim or random forest). 

I think the second part of this question really depends on the detectability of the species. Generally, if the detection rate is high, the true-absence GLM should be much more reliable than the pseudo-absence GLM. If the detection rate is low, I would prefer the pseudo-absence GLM. Additionally, I think to answer which is more reliable, its important to talk about what we are actually modeling. Essentially, the output of these models are a probability that this area is suitable for a species given the environmental covariates used in the model. That is how I would define a SDM, its the probability of suitable habitat based on variables that you can easily measure over large areas. That is why our covariates are elevation, precipitation, mesic forest cover, and canopy cover. Alternatively, I would define an occupancy model as site-specific variables that can only be measured on the ground (also, occupancy models are really to look at finer scale habitat associations and detection probability, but I'm just talking broadly). Anyway, by my definition, reliability is kind of a misrepresentation of SDM performance in this situation. For example, you could look at the overall increased suitability of the new GLM and think that its a result of type 1 error (false positive). Furthermore, you could go to those new "suitable" areas, find no birds, and think that you have confirmed that suitability was inflated (thus proving your suspicions about type 1 errors). However (and I do not know avian ecology at all, so this is based on fish ecology), but those site-specific variables probably affect the local distribution of your species more than the broad-scale covariates (i.e., broad-scale variables may get you to a mountain range [or drainage basin with fishes], but the actual location of the species within that area is probably determined by site-specific variables). So, does the model actually over-predict suitability, or is the broad-scale environment suitable and there is local-scale variables that you were unable to account for? I know that this is an issue with literally every model ever parameterized, but I feel like it is a disproportionately large issue with SDMs. All of that is to say, I would trust the second model until I was given reason not to, then I would trust the next iteration of the model, until that one was improved, and so on.

# Challenge 3 (4 points)

Now plot the relationship between the 4 explanatory variables and the predicted occupancy values based on the two fitted GLM models (presence-background and presence-absence). Recall that we did this in the latter part of our lab. Do you notice any differences in the covariate patterns between the two models? Does this help you interpret the discrepancies between the predicted surfaces from the two models?

```{r}

# Elevation

tmp = expand.grid(elev = seq(min(backCovs$elev), max(backCovs$elev), length=1000),
                  canopy = mean(backCovs$canopy),
                  precip = mean(backCovs$precip),
                  mesic1km = mean(backCovs$mesic1km))

elevData = data.frame(psuedo = predict(glmModel, tmp, type='response'),
                 true = predict(paglmModel, tmp, type='response')) %>% 
  cbind(tmp) %>% 
  select(psuedo:true, elev) %>% 
  pivot_longer(psuedo:true) %>% 
  mutate(variable = 'elevation')

# Canopy

tmp = expand.grid(elev = mean(backCovs$elev),
                  canopy = seq(min(backCovs$canopy), max(backCovs$elev), length=1000),
                  precip = mean(backCovs$precip),
                  mesic1km = mean(backCovs$mesic1km))

canopyData = data.frame(psuedo = predict(glmModel, tmp, type='response'),
                 true = predict(paglmModel, tmp, type='response')) %>% 
  cbind(tmp) %>% 
  select(psuedo:true, canopy) %>% 
  pivot_longer(psuedo:true) %>% 
  mutate(variable = 'canopy')

# Precipitation

tmp = expand.grid(elev = mean(backCovs$elev),
                  canopy = mean(backCovs$canopy),
                  precip = seq(min(backCovs$precip), max(backCovs$precip), length=1000),
                  mesic1km = mean(backCovs$mesic1km))

precipData = data.frame(psuedo = predict(glmModel, tmp, type='response'),
                 true = predict(paglmModel, tmp, type='response')) %>% 
  cbind(tmp) %>% 
  select(psuedo:true, precip) %>% 
  pivot_longer(psuedo:true) %>% 
  mutate(variable = 'precipitation')

# Mesic forest

tmp = expand.grid(elev = mean(backCovs$elev),
                  canopy = mean(backCovs$canopy),
                  precip = mean(backCovs$precip),
                  mesic1km = seq(min(backCovs$mesic1km), max(backCovs$mesic1km), length=1000))

mesicData = data.frame(psuedo = predict(glmModel, tmp, type='response'),
                 true = predict(paglmModel, tmp, type='response')) %>% 
  cbind(tmp) %>% 
  select(psuedo:true, mesic1km) %>% 
  pivot_longer(psuedo:true) %>% 
  mutate(variable = 'mesic1km')

# Manipulaiton


colnames(elevData)[1] = colnames(canopyData)[1] = colnames(precipData)[1] = colnames(mesicData)[1] = 'xValue'

tmp = rbind(elevData, canopyData, precipData, mesicData)

# Plot it

ggplot(tmp, aes(x=xValue, y=value, color=name))+
  facet_wrap(~variable, scales='free_x')+
  geom_line()+
  theme_bw()+
  theme(panel.grid=element_blank())

```

Q. Do you notice any differences in the covariate patterns between the two models? Does this help you interpret the discrepancies between the predicted surfaces from the two models?

Well, obviously the true model has higher predicted suitability values at all covariate values. The trends themselves are the same with the exception of precipitation which changed from exponential to logistic as predicted probability approached 1. I think that this is probably due to a pretty clear difference in these values between present sites and absent sites. In the psuedo model, many of the background points probably had values that overlapped with the present sites, causing less model confidence in the coefficients. I do think that this helps with the discrepancies between the two models because it clarifies that precipitation probably drove the increase in suitable habitat in the second GLM. I also think that this supports the argument that I made in the last question because I interpret these trends as supporting relatively high detectability of this species. If detectability was low, there would be considerable overlap in covariates at present and absent sites, resulting in less model confidence in the coefficients. Alternatively, we see here that the coefficients were relatively high compared to the background points.

# Challenge 4 (4 points)

Varied Thrush are considered forest-dependent, and thus one might characterize mesic forests as "habitat" for the species. Calculate the total amount of mesic forest in the study area, and the mean size of the mesic forest patches.

Using the SDM built from the random forest model, convert the landscape into "habitat" and "non-habitat." To do this, choose a threshold value in your SDM and convert all cells with predicted outcomes greater than this threshold to 1 and all cells with predicted values below your threshold to 0. Justify your choice of your threshold value. Now calculate the total amount of habitat and mean size of habitat patches based on this new raster (i.e., create patches of "habitat" based on aggregations of cells you deemed 1). How do the habitat amount and patch size values compare between the mesic forest approach and the SDM-based approach? In what situations might you rely on one map over the other?

```{r}

# Mesic values

plot(mesic)

lsm_c_ca(mesic, directions = 8) # 4021700

lsm_c_area_mn(mesic, directions = 8) # 749.0594

# Random forest values

predHabitat <- rfMap

plot(predHabitat)

predHabitat[predHabitat < 0.5] = 0
predHabitat[predHabitat >= 0.5] = 1

plot(predHabitat)

lsm_c_ca(predHabitat, directions = 8) # 30380

lsm_c_area_mn(predHabitat, directions = 8) # 11.80264

```

Q. How do the habitat amount and patch size values compare between the mesic forest approach and the SDM-based approach? In what situations might you rely on one map over the other?

I chose 0.5 as my threshold value because 1. I have seen it in several manuscripts before, and 2. I think it makes sense because, according to the model, there is at least an equal probability of suitable habitat versus non-suitable habitat. 

There is more than two orders of magnitude less (i.e., less than 1%) total habitat and mean patch size with the SDM-based approach than the mesic forest approach. I think that both methods can be useful depending on the situation. I would imagine that the mesic forest approach kind of represents the maximum possible habitat, if all other environmental conditions were met, whereas the SDM-based approach represents the actual habitat given the average conditions of the other variables. For example, precipitation was shown to be a very important covariate in our GLMs. In "normal" precipitation years, the SDM-based approach gives you the realized habitat for those specific conditions. However, in an abnormally "wet" year, that model may result in a habitat distribution more closely resembling the mesic forest approach. It kind of ties into what I said in the previous question. Mesic forest may serve as a restricting variable, but more localized conditions, such as precipitation, actually dictates the species distribution within the mesic forest.

# Challenge 5 (4 points)

When we fit the Maxent model in the lab, we used a regularization constant of 1. Fit the model two more times, using regularization (regmult) constants of 0.5 and 3. Construct figures showing the relationship between the 4 explanatory variables and the predicted outcome from these 3 fitted Maxent models. What is the regularization constant doing? Hint: you may need to Google it.

```{r}

# 0.5 Maxent

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel05 = maxnet(p = pbVect,
                     data= covs,
                     regmult = 0.5,
                     classes='lqpht')

plot(maxentModel05, type='logistic')

maxentMap05 = predictMaxNet(maxentModel05, layers, type='logistic')

par(mfrow=c(1,1))
plot(maxentMap05)


# 1 Maxent

maxentModel1 = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')

plot(maxentModel1, type='logistic')

maxentMap1 = predictMaxNet(maxentModel1, layers, type='logistic')

par(mfrow=c(1,1))
plot(maxentMap1)


# 3 Maxent

maxentModel3 = maxnet(p = pbVect,
                     data= covs,
                     regmult = 3,
                     classes='lqpht')

plot(maxentModel3, type='logistic')

maxentMap3 = predictMaxNet(maxentModel3, layers, type='logistic')

par(mfrow=c(1,1))
plot(maxentMap3)


```

Q. What is the regularization constant doing?

The regularization constant is smoothing the response curves by decreasing the coefficients of the model. I've been reading maxent literature lately and, from what I can tell, there is kind of an unlimited number of coefficients associated with each variable through threshold and hinge features (based on sample size, so limited by that, I guess). Regularization penalizes these terms "in proportion to the magnitude of the coefficients" (Merow et al. 2013), so if those seemingly unlimited threshold and hinge coefficients are proportionally small compared to other coefficients, they essentially become 0 and that complextity is removed from the model. I have seen that Maxent assigns default regularization values depending on feature type, so I am curious how giving the model a set value affects this default regularization value. For example, does 0.5, 1, 3 become the regularization value across all feature types, or does it scale the default values accordingly (50%, 100%, 300%). Since I want to do my project using Maxent, I will need to read up on this to justify my choices.


